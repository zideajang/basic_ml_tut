## 决策树

### 决策树定义

### 决策树应用
今天

| No.  |  薪资  |  待遇  |  是否接受offer  |
|---|---|---|---|
| 1  |   1|  1 |   1|
| 2  |   1|  2 |   1|
| 3  |   0|  0 |   0|
| 4  |   0|  0 |   0|
| 5  |   1|  1 |   1|
| 6  |   0|  0 |   0|
| 7  |   1|  2 |   1|
| 8  |   1|  1 |   1|
| 9  |   0|  2 |   0|
| 10  |  1|  0 |   0|
表 1 - 1
- 第一列表示待遇高低 1 表示高 0 表示低
- 标准第二列表示岗位是否有五险一金 1 表示有五险一金 0 如果没有公积金只有五险用 2 来表示
- 1 表示接受 offer 0 表示没有接受 offer


$$ - \frac{1}{2} \log \frac{1}{2} - \frac{1}{2} \log \frac{1}{2} = 1$$
图 1 - 1
```
-(5/10.0 * math.log (5/10.0,2))*2
```
$$ - \frac{1}{6} \log \frac{1}{6} -  \frac{5}{6} \log \frac{5}{6}$$
图 1-2
```
-1/6.0 * math.log(1/6.0,2) - 5/6.0 * math.log(5/6.0,2)
```
$$ 0.65 *  $$

| 分类  | 数量  | 比例  |
|---|---|---|
| 0  |  $\frac{4}{10}$ | $ 0 \rightarrow 4, 1 \rightarrow 0$ |
| 1  |  $\frac{3}{10}$ | $ 0 \rightarrow 0, 1 \rightarrow 3$ |
| 2  |  $\frac{3}{10}$ | $ 0 \rightarrow 1, 1 \rightarrow 2$ |

$$ \frac{3}{10} \times \left( -\frac{1}{3} \log \frac{1}{3} - \frac{2}{3} \log \frac{2}{3} \right) = 0.27 $$

```
>>> -1/3.0 * math.log(1/3.0,2) - 2/3.0 * math.log(2/3.0,2)
0.9182958340544896
>>> 0.9182958340544896 * 0.3
0.27548875021634683
>>> 1 - 0.27
0.73
```

### 什么是决策树
### 创建一颗决策树
### 决策树算法
有许多条件可以对数据进行划分来得到我们想要结果，但是如何划分，根据什么进行划分数据，是我们在设计需要考虑两个问题，我们可以列出，
#### ID3 C4.5



#### CART(C And Regression Tree)
### 信息熵

#### 元素数量对信息熵的影响
$$\frac{1}{3},\frac{1}{3},\frac{1}{3} = 1.58$$
$$\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4} = 2.0$$

```
>>> 3 * (-1/3.0 * math.log(1/3.0,2))
1.584962500721156
```

```
>>> 4 * (-1/4.0 * math.log(1/4.0,2))
2.0
```
#### 概率不均匀的影响
$$\frac{1}{3},\frac{1}{3},\frac{1}{3} = 1.58$$
$$\frac{7}{10},\frac{2}{10},\frac{1}{10} = 1.15$$
```
>>> -7/10.0 * math.log(7/10.0,2) - 2/10.0 * math.log(2/10.0,2) - 1/10.0 * math.log(1/10.0,2)
1.1567796494470395
```
中文是主流语言效率最高的一种语言，根据一位老外计算信息熵所得9.3 而英文才 2. 多，这个应该不难理解。我可以通过信息熵知识帮助我们了解到为什么中文是一种高效的语言。其实单位时间每种语言所传递的信息量都基本差不多

我们平时使用RAR来对文件压缩，压缩过程中去掉文本中冗余，其中压缩率中文是
#### 
### 条件熵
### 互信息

### 条件熵推导的定义
$$ \sum_x p(x) \sum_y p(y|x) \log p(y|x)$$
### 相对熵
$$D(p||q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = E_{p^(x)} \log \frac{p(x)}{q(x)}$$
### 互信息
$$ I(x,y) = D(P(x,y)||P(x)P(y))$$
到底有多大距离，条件熵意义就是度量两个随机变量间距离
$$I(X,Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$
独立时候条件
$$  \frac{p(x,y)}{p(x)p(y)} = 1$$
$$ \log \frac{p(x,y)}{p(x)p(y)} = 0$$

#### 计算条件熵的定义的推导
$$ H(X) - I(X,Y) = H(Y|X) $$
#### 总结
$$ I(X,Y) = H(X) + H(Y) - H(X,Y) $$
- 可以尝试推导一下
- 证明
$$ H(X|Y) \le H(X) $$
$$ H(Y|X) \le H(Y) $$
### 
$$ KL(p||q) = p(x) \log \frac{p(x)}{q(x)}$$
$$ = p(x) \left[ \log p(x) - \log q(x) \right]$$
$$ = p(x) \log p(x) - p(x) \log q(x)$$
$$ = - \left[ -p(x) \log p(x) + p(x) \log q(x) \right]$$
$$ H(p) + \log q(x)^p $$
### 决策树算法
指数越大，表明属性对样本熵减少
#### 决策树
同样使用贪心法(和梯度下降一样)
#### ID3
对模型特征有若干个$f_1,f_2,f_3, \dots f_n$然后我们按某一个特征进行划分，在给定$f_1$条件，
$$H(X) - H(X|f_1)$$
对于 f2 给定
$$H(X) - H(X|f_2)$$
谁下降越多谁就胜出，这就是信息增益。
#### C4.5
选择 id 进行分类，可能分类特别多(也就是该节点)有过多分支，那么就很容易，那么每一个分支一定确定因为只有一个样本，甚至每一个样本，问题分类数目过多，所以id分类本身熵比较大。
考虑本身的熵
$$ 10 * (-\frac{1}{10} \log \frac{1}{10})  $$
除以其本身信息熵
#### CART
Gini系数
$$Gini(p) = \sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^K p_k^2$$
$$ = 1 - \sum_{k=1}^K \left(  \frac{|C_k|}{|D|} \right)^2$$
注意在机器学习中基尼系数与经济学中用于计算贫富差异的基尼系数是不同的，如果大家感兴趣
度量社会贫富差异，$ ln  $ 图像
在 1 的点$\ln x$ 斜率是 1，
$$ y = -x + 1 (x \in [0,1])$$
用直线段代替曲线，就可以，gini 就是log x熵一阶近似
将f(x) = -lnx 在 x=1 处进行一阶展开，忽略高阶无穷小就得到f(x) = 1-x 
#### 评价函数
$$C(T) = \sum_releaf N_t \cdot H(t)$$
### 决策树过拟合
可以通过随机森林预防过拟合，在树某一个分支很长，
在做第二个就会不在这里过拟合，如果我们做 n 棵树，不用在考虑树过拟合，所谓过拟合就是我们学到数据的噪声，
#### Bootstraping
提升自己而不借助外力，
从样本集中重采样(有重复的)选出n个样本，在所有属性上，对这 n 个样本建立分类器
保证样本不一样，从 1 到 N 个随机数。
一颗决策树是弱分类器，有放回的生成样本，
SVM 强分类器，有时候和 bagging 这样结合有时候并没有



我们假设有 N 样本然后在 N 样本中进行有放回的选取出 N 个样本 N1，这样就有三种情况，
1. 有些样本被选中一次
2. 有些样本被选中多次
3. 有些样本一次也没有被选中

我们计算在 N2 出现过样本大概数量，假设一个样本只要出现过我们将其计数为 1 然后计数为 N2 那么 $N2 \le N$
任何样本被选中概率和没有被选中概率如下
$$ \begin{cases}
    \frac{1}{N}  \\
    (1- \frac{1}{N})
\end{cases}$$
那么我们一个样本在 N 选择中没有被选中的概率就是
$$(1 - \frac{1}{N})^N$$
用 1 减去在 N 次没有被选中的概率就得到样本被选中的概率
$$  \lim_{N \rightarrow 0} \left[ 1 - (1 - \frac{1}{N})^N \right]$$
$$ \lim_{N \rightarrow 0 } \left[ 1 - [(1 + \frac{1}{-N})^{-N}]^{-1} \right] $$
$$ \frac{1}{1 - e} \approx 63.2 \% $$
那么也就是说明大概有 63.2% 数据可以进入分类器，36.8% 没有被包括到分类器中。

#### OOB(Out of Bag)
袋子外数据用于测试决策树估计误差，所以在决策树时候我们无需将数据分为测试集和训练集数据。
有关我们每一次采样数量没有必要给，1/2 采样率?样本规模没有必要
$$ \alpha N$$

一共有 m 个样本， n 个特征进入 $m \prime = 0.75 m$ 其实 n 
$$ m  \times n $$