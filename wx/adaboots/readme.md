之前我们介绍过的决策树就是一种弱分类器，我们可以通过 adaboots 建立多个弱分类器，然后对分类器加权重后联合在一起形成模型要优于单个分类器。

我们这里用一个一层决策树，也就是深度为 1 ，只能根据一个特征只对数据进行分类和划分

### Adaboots 的推导
假设我们有一个弱分类器 $h(x) h(x) \in \{ -1, 1\} $,
$$ H(x) = sign(\sum_{i=1}^T h^t(x)) $$
其实这个公式不难理解 H 表示T 个弱分类器组合而成强分类器,我们是把 N 弱分类器的结果相加后取符号，也就是如果多个分类器结果为负数那么H就是 -1 反之亦然。
这样做看起来有点像大锅饭，我们需要给每一个弱分类器一个权重，这样让表现好的分类器对估计结果影响大一些。

$$ H(x) = sign(\sum_{i=1}^T \alpha^t h^t(x)) $$

这里说一些多个弱分类器是串行的计算而不是并行的计算，我们为什么要用串行，这是因为下一个分类器是需要专注上一个分类器分类错误的样本。

我们这里假设数据样本是$\{(x_1,y_1),(x_2,y_2),\dots , (x_i,y_i) \} y_i \in \{ -1,1 \}$

我们知道每一次迭代弱分类器关注不同样本点，那么如何确定我们每一个弱分类器关注的点不同。我们就需要给每一个数据样本添加一个权重，每一个弱分类器根据样本的权重来决定是否关注这个样本。
$$w_i^t = \frac{1}{N}$$

$$\epsilon = \sum_{y_i \ne h_i(x)} \frac{1}{N} \epsilon \in [0,1]$$

$$\epsilon = \sum_{y_i \ne h_i(x)} w_i \sum w_i = 1$$
那么我们现在如何确定让下一个弱分类器关注那些样本点。

$$w_i^{t+1} = \frac{w_i^t}{Z} e^{-\alpha^t h^t(x_i)y_i}$$

$$w_i^{t+1} = \begin{cases}
    \frac{1}{Z} w_i^t e^{-\alpha^t} & y_i = h^t(x_i) \\
    \frac{1}{Z} w_i^t e^{\alpha^t} & y_i \ne h^t(x_i) \\
\end{cases}$$