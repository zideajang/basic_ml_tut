## 逻辑回归
使用线性回归来解决分类问题，也是我个人碰到的第一个难懂的模型。之前我们一直注重代码实现而总是忽略其背后的原理和算法。机器学习不同与写 web 和服务，这些可能更注重实现，了解业务和计算机原理。而机器学习更注重的是其背后算法，只有了解其背后原理我们在能够设计出合理模型来解决问题。随后通过调整参数来调试我们模型来解决问题。
#### 逻辑回归的重要性
逻辑回归是深度学习的基础，通常来说深度学习对于我们来说是一个黑盒模型，有时候并不能完全了解其。我们知道在神经元就是将线性变换转换为非线性变换。其实这些知识点我们都可以在逻辑回归中找到。逻辑回归的本质就是将线性回归通过非线性函数。

### 二项逻辑回归
- **二项逻辑回归**是一个函数，最终输出 0 到 1 之间的值，可以解决类似"成功或失败","是否存在"或是"通过或拒绝"等类似的问题
- **逻辑回归**是一个把线性回归问题模型映射为概率模型，即把实数空间输出$[- \infty,+\infty]$ 映射到(0,1),从而获得概率。
- 对于一元线性方程模型可以表示为 $\hat{y} = wx + b$ 其中 b 为截距通过改变 b 值可以上下或左右移动直线，w 改变直线斜率。
- 定义二元一次线性模型 $\hat{y} = \theta_1 x_1 + \theta_2 x_2 + b$ 我中 $\hat{y}$ 取值范围在 $[- \infty,+\infty]$ 我们可以通过逻辑回归将$(-\infty,+\infty)$ 映射到$[0,1)$区间

### 概率和odds的定义
#### 概率
概率是事件发生次数/总次数，这个很好理解。
$$p = \frac{事件(正例样本)发生次数}{总次数} p \in [0,1]$$
#### odds
$$\frac{正例样本数量}{负例样本数量} odds \in [0,\infty)$$ 
### 伯努利分布
简单地回顾一下伯努利分布，X 是伯努利分布中的随机变量，X 取值为$\{ 0,1 \}$，那么我看一看概率情况
$$\begin{aligned}
    P(X=1) = p \\
    P(X=0) = 1 - p
\end{aligned}$$
伯努利公式也可以写出$p^k(1-p)^{(1-k)}$
$$ odds = \frac{p}{1-p} odds \in [0,+\infty)$$
### sigmoid 函数及特性
对于$\frac{p}{1 - p}$ 取 log 那么就得到 $\log \frac{p}{1 - p} $ 函数输出就从0 到 1 就变为到 $[-\infty,+\infty]$ 现在可以用$\log \frac{p}{1-p}$ 来等价于上面定义二元一次方程$\theta_1 x_1 + \theta_2 x_2 + b$

$$ 
\begin{aligned}
    \log \frac{p}{1 - p} = z \\
    \theta_1 x_1 + \theta_2 x_2 + b = z  \\ 
    e^z = \frac{p}{1-p} \\
    (1-p) e^z = p \\
    e^z = p + p e^z \\
    e^z = p + p e^z \\
    p = \frac{e^z}{1+e^z} \\
    p = \frac{1}{\frac{1}{e^z}+1} \\
    p = \frac{1}{e^{-z}+1}
\end{aligned}$$

上面经过一系列的推导就得到了这个$p = \frac{1}{e^{-z}+1}$ 大家熟悉的 sigmoid 方程。也就是将一个实数空间的解映射成概率，类似于神经元激活函数。
我们都知道正是最大似然估计推动机器学习的发展，那么今天我们来看一看最大似然估计是如何在逻辑回归中发挥其作用的。
我们其实总是能够得到样本数据，样本数据的属性通常会服从正太分布,$\mu,\sigma$ 均值和方差决定数据形态，那么如果我们知道类别的概率分布也就是$\mu,\sigma$，再得到新的样本点，就可以根据概率分布来判断他是某一个类别的概率是多少从而判断他是什么类别。

最大似然估计就是找到符合样本数据概率分布的$\mu,\sigma$ 然后我们根据这些来进行推断。

##### 似然函数
$$ \begin{aligned}
    L(\theta|x) = P(Y| X;\theta) \\
    = \prod_i^m P(y_i|x_i;\theta) \\
    = \prod_i^m h_{\theta} (x_i)^{y_i}(1 - h_{\theta}(x_i))^{(1-y_i)}
\end{aligned} $$

其中 i 为每一个数据样本，共有 m 个数据样本，**最大似然估计**的目的就是让上面式子的输出尽量可能大;对上面式子取 log 是为了方便计算，因为取log可以把乘积运算转换为加法，而且不影响优化目标。
$$L(\theta|x) = P(Y| X;\theta) = \sum_{i=1}^m y_i \log(h_{\theta}(x_i)) + (1-y_i) \log(1-h_{\theta}(x_i))$$
对于似然函数我们是让越大越好，不过我们计算损失函数希望计算损失函数时候希望是一个求最小值的问题，加一个负号就变成求最小问题。然后求导数就可以找到这函数的最最小值
$$J(\theta) = -\sum_i^m Y \log(\hat{Y}) - (1 - Y) \log (1 -\hat{Y})$$

### 交叉熵
$$CrossEntropy(Y,\hat{Y}) = - \frac{1}{m} \sum_i^m \sum_c^{N_c} Y \log(\hat{Y})$$
在深度学习中说到的交叉熵，和最大似然估计是一回事，只不过上式交叉熵中可以从二项分布扩展到多分类问题。

### 困惑度(perplexity)
通过了解困惑度，可以让我们更加深入的理解交叉熵意义，也可以轻易量化模型的性能。
我们首先来看一下这个交叉熵公式
$$crossEntropy(Y,\hat{Y}) = - \sum_{c}^{N_c} Y \log(\hat{Y})$$

- 上面式子中 c 为分类编号
- $N_c$ 为所有的分类数量
- 其中 Y 表示真实值，使用独热编码形式表示
- 其中$\hat{Y}$ 表示估计值

那么假设 $Y = [0,0,1]^T$ 其意义是样本集合中有 3 种类别，而当前样本属于第 3 类，因为向量前两位都是 0 只有第 3 位是 1。矩阵为$[\hat{y_1},\hat{y_2},\hat{y_3}]$ 这里是通过模型计算出对样本的所属类别估计概率分布，他们概率和为 1

那么这将值带入交叉熵公式，因为 0 乘以一个值为 0 忽略前两项，我们重点看一下第 3 项，如果$\hat{y_3}$ 值越大也就是越接近 1 那么取log 就是 0 所以只要我们估计值越接近真实交叉熵就近视为 0 。（分类错误）

$$J(\theta) = - \sum_c^{N_c} Y \log(\hat{Y}) = 1 * - \log(\hat{y}_c)$$


同时满足这两个限制条件的最优化问题称为**凸优化问题**，这类问题有一个非常好性质，那就是局部最优解一定是全局最优解。接下来我们先介绍**凸集**和**凸函数**的概念。
$$\theta x + (1 - \theta) y \in \mathbb{C}$$
![](https://github.com/zideajang/basic_ml_tut/blob/master/wx/logistic_regression/screenshots/context_1.JPG)
则称该集合称为凸集。如果把这个集合画出来，其边界是凸的，没有凹进去的地方。直观来看，把该集合中的任意两点用直线连起来，直线上的点都属于该集合。相应的，点：

$$ f(\theta x + (1 - \theta y)) \le f(\theta x) + f((1 - \theta) y)$$


$$ f(x,y,z) = 2x^2 - xy + y^2 - 3z^2$$

$$ \left[ 
 \begin{matrix}
    \frac{\partial^2f}{\partial x^2} && \frac{\partial^2f}{\partial x \partial y} && \frac{\partial^2f}{\partial x \partial z} \\
    \frac{\partial^2f}{\partial y \partial x } && \frac{\partial^2f}{\partial y^2 } && \frac{\partial^2f}{\partial y \partial z} \\
    \frac{\partial^2f}{\partial z \partial x} && \frac{\partial^2f}{\partial z \partial y} && \frac{\partial^2f}{\partial z^2}
\end{matrix} 
\right]
$$

1. Hessian 矩阵正定，函数在该点有极小值
2. Hessian 矩阵负定，函数在该点有极大值
3. Hessian 矩阵不定，还需要看更高阶的导数

#### 局部最优解与全局最优解
对于一个可行点 x，如果在其邻域内没有其他点的函数值比该点小，则称该点为局部最优，下面给出这个概念的严格定义：对于一个可行点，如果存在一个大于 0 的实数 $\delta$，对于所有满足：

$$||x - z ||_2 \le \delta $$
即x的 $\delta$ 邻域内的点 z，都有
$$f(z) < f(x)$$
则称x为局部最优点。对于一个可行点x，如果可行域内所有点z处的函数值都比在这点处大，即:
$$f(z) < f(x)$$
则称x为全局最优点，全局最优解可能不止一个。凸优化问题有一个重要的特性：所有局部最优解都是全局最优解。这个特性可以保证我们在求解时不会陷入局部最优解，即如果找到了问题的一个局部最优解，则它一定也是全局最优解，这极大的简化了问题的求解。下面证明上面的结论，采用反证法，具体证明如下：

假设x是一个局部最优解但不是全局最优解，即存在一个可行解y，有：


根据局部最优解的定义，不存在满足
$$ z= \theta x + (1 - \theta) y $$
$$ \theta = \frac{\delta}{2 ||x-y||_2} $$