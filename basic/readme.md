
- 监督学习: 有标准答案的试错学习
- 无监督学习:根据一定的假设寻找数据内部的结构

我们机器学习主要是研究数据
我们有 N 个样本，每一个样本维度为 p
可以这样表示 $X = (x_1 x_2 \cdots x_N)^T$

### 看电影学贝叶斯
![bays_02](https://upload-images.jianshu.io/upload_images/8207483-a9c0fcc95bebbd39.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
《寄生虫》是是由奉俊昊执导，宋康昊、李善均、赵茹珍、崔宇植、朴素丹等主演的剧情片。
影片讲述住在廉价的半地下室出租房里的一家四口，原本全都是无业游民。在长子基宇隐瞒真实学历，去一户住着豪宅的富有家庭担任家教后，一家人的生活渐渐起了变化。

![bays_01.jpg](https://upload-images.jianshu.io/upload_images/8207483-594ee0819b321108.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
这里有段情节就是女儿为了父亲能够得到富商家司机位置，将自己内裤放到车上来栽赃陷害富商的现任司机，让其下岗。我们通过贝叶斯来帮助富商来做出判断。

我们贝叶斯代表主观概率，是我们对可能性的估计。最大好处就是有了新的信息可以更新概率。
我们通过贝叶斯司机和女孩在富商车上啪啪的概率，
- 在车上发现女孩内裤（证据）发现内裤事件 A
- 求解的是司机和女孩在富商车上啪啪的概率 P(B)
- 先验概率：是富商在没有发现内裤情况下认为司机在自己车上概率为 假设这个事件概率为 0.3 吧
- 后验概率 就是 P(B|A)

$$ P(B|A) =  \frac{P(A|B)P(B)}{P(A)} \Rightarrow  \frac{P(A|B)P(B)}{P(A) + P(A|\overline B)P(\overline B)}$$
- $P(A|B)$ 表示司机在车上和女孩啪啪出现内裤可能，这样可能性不高我们假设 0.1
- $P(A|\overline B)$ 表示司机被陷害的概率为 0.1
$$ \frac{0.1 * 0.3}{ 0.3 * 0.1 + 0.7 * 0.1} = \frac{0.03}{0.1} = 30% $$
如果富商学习过贝叶斯也就会做出正确判断，开除司机从而阻止剧情的发展




概率模型
$$ x \approx p(x|\theta) $$
### 概率派和朴素贝叶斯派
#### 概率
推动概率论

概率相比大家都学习过，但是大家可能还不知道概率背后是可**重复性**。我们还是拿最简单最经典的示例，也就是投硬币大家都知道只要我们做足够多一次然后进行统计就会发现出现正面和背面概率分别都是 50%
我好那么什么是**可重复**性，合适因为如果我们进行多轮投币测试，每轮测试 1000 次，发现每一轮下来正面和反面出现次数比例1比1，这就是概率的**可重复性**。如果没有可重复性概率也就是没有意义。
所以我们发现一些事物背后是一定稳定性的东西(也就是模式)，这是我们机器学习基础。

下面介绍一些概率出现术语和一些基本计算公式来帮助我们更好理解要机器学习。我们补充一些基础知识都是为了机器学习
#### 术语
- 实验: 我们要研究内容，例如投硬币
- 事件: 试验的结果就是事件，也就是投硬币后正面就是事件，用于 A B 大写字母表示。事件是可以组合
- 概率空间：就是所有事件集合表示
#### 概率运算
1. $A \bigcup B$ : $P(A+B)$ 
2. $A \bigcap B $ : $P(A,B)$ 这里两个事件是独立 $P(A,B) = P(A)P(B)$
3. $ \overline{A} $:$P(\overline{A})  = 1 - P(A) $
4. $P(A|B)$: $P(A|B) = P(A,B)/P(B) $
5. $ P(A) = P(A|B)P(B) + P(A| \overline B)P(\overline B)$

### 贝叶斯三要素

#### 推导贝叶斯公式
$$ P(A,B) = P(B,A) \Rightarrow P(A|B)P(B) = P(B|A)P(A) $$
$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$
$$ P(A|B) = \frac{P(B|A)P(A)}{P(A)P(B|A) + P(\overline A)P(B|\overline A)} $$

- P(A) 先验概率
- P(A|B) 后验概率
- B 证据
- A 事件

P(A) 什么都不知道我们对 A 事件发生概率估计，随后得到 B 证据，这样有了证据就增加对 A 发生推断后验概率 P(A|B)

贝叶斯公式的内容（根据数据(证据)更新对事件可能性的估计）这就就是大数据被后支撑理论。


#### 练习
![monty_hall_problem](https://upload-images.jianshu.io/upload_images/8207483-3a9c3cdc545cb402.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
三门问题（Monty Hall problem）亦称为蒙提霍尔问题、蒙特霍问题或蒙提霍尔悖论，大致出自美国的电视游戏节目Let’s Make a Deal。
    问题名字来自该节目的主持人蒙提·霍尔（Monty Hall）。参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车，选中后面有车的那扇门可赢得该汽车。当参赛者选定了一扇门，但未去开启它的时候，节目主持人开启剩下两扇门的其中一扇，里面当然没有汽车。主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机会？
##### 错误答案
根据常识我们会认为概率是 1/2

##### 答案
那么答案是会。不换门的话，赢得汽车的几率是1/3。换门的话，赢得汽车的几率是2/3。

##### 推导
我们看一看本质就是主持开门这个事件是不是影响到之前我们认为 1/2 概率的事件。也就是我们之前提到证据。
![monty_hall_problem](https://upload-images.jianshu.io/upload_images/8207483-3a9c3cdc545cb402.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
我们用 $A_1,A_2,A_3$ 三个事件分别表示 1 号 2 号和 3 号门有车事件概率，那么我们每一个事件概率都是 $A_n = \frac{1}{3}$

- 参加者选择 1 号门
- 开门前的**先验概率**为 $ A_1 = 1/3 $
- 然后主持人开 3 号门，主持人开门事件就是证据, 用事件 B 来表示
  - 如果 1 号门有车主持打开 3 号门概率是 1/2
  - 如果 2 号门有车主持人打开 3 号门概率 1
  - 如果 3 号门有车主持人打开 3 号门概率为 0
$$ P(A_1|B) = P\frac{P(B|A_1)P(A_1)}{P(B)} \Rightarrow \frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + P(B|A_3)P(A_3)}$$

$$ \frac{(\frac{1}{2} * \frac{1}{3})}{ 1 * \frac{1}{3} + \frac{1}{2} * \frac{1}{3} + 0 * \frac{1}{3} }  = \frac{2}{3}$$

### 辛普森杀妻按键


### 连续变量和分布函数
#### 高斯分布
#### 贝叶斯推理深入
1968 年就有案例通过沉船，采用贝叶斯算法进行搜索。
with probobility map of possible location of the scorpion

#### 机器学习
##### 过拟合
##### 贝叶斯错误率
#####
概率想必大家在大学里都学习过，但是大家可能还不知道概率背后是支撑是可重复性。我们还是拿最简单，最经典的示例给予概率是可重复的进行说明，这个例子就是大家熟知的投硬币实验。大家都知道只要我们做足够多次实验，然后进行统计就会发现出现正面和背面概率分别都是 50%
​好那么什么是可重复性，我们进行多轮投币测试后，假设每轮测试 1000 次，发现每一轮下来正面和反面出现次数比例1比1，这就是概率的可重复性，如果没有可重复性概率也就是没有意义。


所以我们发现一些事物背后是一定稳定性的东西(也就是模式)，这是也是之说以机器能够通过训练进行学习的缘故。


#### 监督学习
#### 无监督学习




## PCA 算法





### 多元线性回归
之前我们完整地介绍了什么是一元线性回归问题，以及通过梯度下降来求解到损失函数最小时所对应的参数。从而找到一元线性回归的最优解。今天我们将问题扩展到多元回归问题。在回归分析中，如果有两个或两个以上的**自变量**，就称为多元回归。
在回归分析中，如果有两个或两个以上的**自变量**，就称为多元回归。
多元线性回归与一元线性回归类似，可以用**最小二乘法**估计模型参数，也需对模型及模型参数进行统计检验。
### 多元线性回归模型
$$h(\theta_1,theta_2, \cdots \theta_m) = \theta_0 + \theta_1x_1  \theta_2x_2 + \cdots \theta_mx_m$$
之前我们了解到了多元线性回归是用线性的关系来拟合一个事情的发生规律,找到这个规律的表达公式,将得到的数据带入公式以用来实现预测的目的,我们习惯将这类预测未来的问题称作回归问题.机器学习中按照目的不同可以分为两大类:回归和分类.今天我们一起讨论的逻辑回归就可以用来完成分类任务.
$$ g(x) = \frac{1}{1 + e^{-x}} $$

sigmoid

$$ h_{\theta}(x) = \frac{1}{1 + e^{-\theta^TX}} $$

$$ h_{\theta}(x) = \theta_0 + \theta_1 x$$

$h$ 表示函数 $\theta_0$ 表示截距(以后叫偏移值) $\theta_1$ 表示斜率（以后叫权重)

$$ h_{\theta}(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_n x_n$$ 


多元线性回归
$$ h_{\theta} = \theta^Tx= \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_n x_n$$
这里解释一下参数其中 $\theta_0,\theta_1 \dots \theta_n $ 这里$\theta^T$是向量转置，也就是行列变换，有 n 行 1 列向量变换为 1 行 n 列。
接下里看一下他的损失函数，没有什么不同就是在一元线性回归基础
$$ J(\theta_0,\theta_1 \dots \theta_n) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2$$
接下来就是我们通过梯度下降法来优化我们参数来达到损失函数最小值
$$ \theta_j := \theta_j - \eta \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1 \dots \theta_n)$$
分类准确率
最大似然估计法(maxiumu likelihood)
**似然性**就是概率，也就是我们假设的使我们数据最大，也就是在这个假设下数据出现的最大。

$$P(D|\theta)$$
$$P((x_1,y_1),(x_2,y_1),\dots,(x_n,y_n)|\theta)$$
联合分布也就是所有数据出现的可能形式
$$P((x_1,x_2,\dots,x_n),(y_1,y_2,\dots,y_n)|\theta) $$
$$P(\vec{Y},\vec{X} | \theta) $$
### 离散型随机变量
设总体 $X$ 是离散型随机变量，概率分布为 $P\{ X = t_i \} = p(t_i;\theta),i = 1,2, \dots$ 其中 $\theta \in \Phi$ 为带估计参数。
设$X_1,X_2, \dots ,X_n$ 是来自总体样本 X 的样本，$x_1,x_2, \dots,x_n$ 是样本值，成函数$ L(\theta) = L(x_1,x_2, \cdots , x_n;\theta) = \prod_{i=1}^n p(x_i;\theta) $ 为样本 $x_1,x_2, \dots x_n$ 的似然函数，如果$\theta \in \Phi$ 使得 $L(\hat{\theta}) = \max_{\hat{\theta} \in \Phi} L(\theta)$ 这样 $\hat{\theta}$ 与 $x_1,x_2, \dots,x_n$  有关，记作$\hat{\theta}(x_1,x_2, \dots,x_n )$,称未知参数 $\theta$ 的最大似然估计值，相应的统计量$\hat{\theta}(x_1,x_2, \dots,x_n )$ 称为 $\theta$ 的最大似然估计量。
### 连续型随机变量
设总体 $X$ 的概率密度函数 $f\{ X = t_i \} = p(t_i;\theta),i = 1,2, \dots$ 其中 $\theta \in \Phi$ 为带估计参数。
设$X_1,X_2, \dots ,X_n$ 是来自总体样本 X 的样本，$x_1,x_2, \dots,x_n$ 是样本值，成函数$ L(\theta) = L(x_1,x_2, \cdots , x_n;\theta) = \prod_{i=1}^n p(x_i;\theta) $ 为样本 $x_1,x_2, \dots x_n$ 的似然函数，如果$\theta \in \Phi$ 使得 $L(\hat{\theta}) = \max_{\hat{\theta} \in \Phi} L(\theta)$ 这样 $\hat{\theta}$ 与 $x_1,x_2, \dots,x_n$  有关，记作$\hat{\theta}(x_1,x_2, \dots,x_n )$,称未知参数 $\theta$ 的最大似然估计值，相应的统计量$\hat{\theta}(x_1,x_2, \dots,x_n )$ 称为 $\theta$ 的最大似然估计量。

我们之前学习回归函数$h_{\theta}(\theta^Tx)$ 样子想必大家已经都熟悉了吧。$g(x) = \frac{1}{1+e^{-x}}$
我们把$h_{\theta}$作为变量带入g(x) 中就得到今天逻辑回归的模型
$$ h_{\theta} = \frac{1}{1 + e^{- \theta^TX }} $$
我们之前学习回归函数$h_{\theta}(\theta^Tx)$ 样子想必大家已经都熟悉了吧。$g(x) = \frac{1}{1+e^{-x}}$
我们把$h_{\theta}$作为变量带入g(x) 中就得到今天逻辑回归的模型
$$ h_{\theta} = \frac{1}{1 + e^{- \theta^TX }} $$

我们已经知道逻辑回归是用来做分类的，一般分为 2 个 类别，我们将 0.5 作为分类边界。sigmoid 函数特点就是 0 到 1 我们就可以小于 0.5 归为一个类别。
- 当$z\ge0$ 时候$g(z) \ge 0.5$ $z\ge0$ 也就是 $\theta^TX \ge 0$ 所以这时$g(\theta^TX) \ge 0.5$
- 当$z\le0$ 时候$g(z) \ge 0.5$ $z\le0$ 也就是 $\theta^TX \ge 0$ 所以这时$g(\theta^TX) \le 0.5$

### 决策边界
$$ h_{\theta}(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_2^2 + \theta_4x_2^2  ) $$

### 损失函数

$$ J(h_{\theta}(x),y) = \begin{cases}
  -\log(h_{\theta}(x)) & y = 1 \\
  -\log(1 - h_{\theta}(x)) & y = 0
\end{cases} $$

$h_{\theta}(x)$接近于 1 时候**损失函数**就为 0
我们是无法对分段函数进行求导，不过这里用了一个小技巧来解决这个问题。
$$ J(h_{\theta}(x),y) = -y\log(h_{\theta}(x)) - (1-y)\log(1 - h_{\theta}(x))$$
- 当$y=1$时，$J(h_{\theta}(x),y) = -\log(h_{\theta}(x)) $
- 当$y=0$时，$J(h_{\theta}(x),y) = -\log(1 - h_{\theta}(x))$
$$
\begin{cases}
  \theta_0 = \theta_0 - \eta \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)}) \\
  \theta_1 = \theta_1 - \eta \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}
\end{cases}
$$

### 梯度下降
$$ J(\theta) = -\frac{1}{m}[\sum_{i=1}^m y^{(i)}\log h_{\theta}(x^{(i)}) + (1-y^{(i)})\log(1-h_{\theta})\log(1-h_{\theta}(x^{(i)})) ]$$

$$ 2x \rightarrow 2 $$
$$ x^2 \rightarrow 2x $$
$$ \frac{\partial g(f(x))}{\partial}  \rightarrow \frac{\partial g(f(x))}{\partial f(x)} \frac{\partial f(x)}{\partial x} $$

$$ \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \rightarrow 2 \cdot \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) \rightarrow \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) $$

$$ \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 \rightarrow 2 \cdot \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) \rightarrow \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)} $$


### 无监督学习和监督学习
我们现在研究大多数都是

$$ \frac{1}{1 + e^{x}} $$

线性回归(linear regression)是用来做回归(预测)的，逻辑回归(logistic regression)是用来做分类的。虽然两者用于

解决不同问题，但是因为名字里都有回归所以还是有着一定关系不然就叫逻辑分类。
### 伯努利分布
伯努利分布(Bernoulli distribution)又名两点分布或 0-1 分布，在介绍伯努利分布前先介绍一下**伯努利实验**
**伯努利试验**是只有两种可能结果的单次随机试验，即对于一个随机变量X而言。
进行一次伯努利试验，成功(X=1)概率为p(0<=p<=1)，失败(X=0)概率为1-p，则称随机变量X服从伯努利分布。
$$P(y=1|x,\theta) = h_{\theta}$$
$$P(y=0|x,\theta) = 1 - h_{\theta}$$
### 最大似然估计
根据数据寻找一个出现这样数据概率最高的参数就是最大似然估计。也是机器学习的过程。最大似然估计法背后是最小经验误差。就是在训练数据集上找一个让我们训练数据集误差最小的假设。
$$ \max P(Y|X,\theta) $$
最大似然估计的意思就是最大可能性估计,其内容为:如果两件事 A,B 相互独立,那么A和B同时发生的概率满足公式
$$ P(A,B) = P(A) \cdot P(B)$$

### 过拟合
最大似然估计只是考虑现有数据出现概率最大，就是y = f(x)相当于经验误差最小，如果你投硬币10次9次为正，但是我们下一次猜正概率，只要出了训练集，我们训练模型就会变的很差。所以我们最求泛化误差最小。
如果对应较少数据我们选择了更多参数来拟合这个数据，每一个参数$P(w_1) \cdots P(w_2)$ 参数也多我们选择模型就越多，如果选择简单模型，结构简单就两个参数，因为数据中一定有噪声，一定我们容量（参数）大模型完美拟合训练集也也就是。

### MAP(maximum a posterior)
最大后验估计，贝叶斯需要考虑先验概率，放入先验概率好处是增强稳定性。
- 教条主义
- 经验主义
生活也是这样，既要借鉴前人经验，又要相信数据。




### 训练集和测试集
### 梯度下降
梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降(Gradient Descent)是最常采用的方法之一，另一种常用的方法是最小二乘法。除了梯度下降我们随后还会介绍其他优化算法。不多神经网络用的。
梯度下降是一种优化算法，
$$ w_{(i+1)} = w_i - \eta \frac{\partial L(w)}{ \partial w}$$
我们初始化参数 w 然后按某个方向在损失函数移动 w 移动距离是由损失函数对 w 导数和一个学习率的乘积。如果变化率小于 0 。

问题
局部最小值

学习率使用，
- 如果学习率设置过小，每次只前进一小步这样缓慢下去可能求失去耐心了
- 如果学习率设置过多，我们可能一步就越过谷底，在谷底附近进行震荡而始终无法达到谷底。
- 随后我们会根据训练步骤以及不同参数不断调整学习率，现在只要知道学习率是比较关键的一个参数即可。以后会不断更新我们对学习率认识。

### 概率派和贝叶斯派(Bayesian)
- 数据集 $$ X:  {x^1, x^2 \cdots x^i }$$
- 参数 $$ \theta $$


### 统计学研究方向
什么是统计是关于数据的学科，我们想要增加对大自然了解。搜集数据然后对自然进行推断。我们日常计算均值和方程并不是统计学研究问题，统计学主要研究统计推断。统计学主要两个学派——频率学派和贝叶斯学派
- 频率学派
知道参数的点估计，然后再告诉这个点估计到底有多大准确度。
- 贝叶斯学派
贝叶斯脱离统计的，统计模型、线性模型或logist模型，对于模型参数进行进行预测，在贝叶斯得到分布，有一个先验条件，在贝叶斯对参数进行推断然后得到参数的分布。
我们假设一个样本是根据工作岗位的条件一名程序员是否接收工作的

| 接收offer | 常出差 | 加班 |  高工资 |
| ------ | ------ | ------ | ------ |
| 0 | 1 | 1 |0 |
| 1 | 0 | 0 |1 |
| 1 | 1 | 0 |1 |
| 0 | 0 | 1 |0 |
| 0 | 0 | 1 |1 |
| 1 | 0 | 1 |1 |

- A  表示接受offer  
- B 表示常出差 
- C 表示加班 
- D 表示高工资
如果一个人接收一个岗位（没有加班，没有加班高工资的岗位）接收概率。
从样本来看 $ P(A=1) = \frac{1}{2} $ 表示接受 offer 可能性为 $ P(A=0) = \frac{1}{2}$
我们在看一看常加班概率 $P(B=1) = \frac{1}{3} $ 和 $P(B=0) = \frac{2}{3}$
$P(A\bigcap B) = \frac{1}{6}$
在条件概率也就是 B已经发生了 A发生概率用 $P(A|B)$ 来表示。在已经知道加班的条件接收offer的概率，看表是 $\frac{1}{2}$
我们看已知加班接收offer概率是 $\frac{1}{4}$。说明大多数人都讨厌加班。
然后我们看看高工资条件下接收 offer 概率是$\frac{3}{4}$

$$ P(A|B) = \frac{P(A \bigcap B)}{P(B)} $$
$$ P(A|B)P(B) = P(A \bigcap B) $$

如果已知 A 然后 B 的概率
$$ P(B|A) = \frac{P(A \bigcap B)}{P(A)} $$
$$ P(B|A)P(A) = P(A \bigcap B) $$
然后我们将上面两个公式进行变换后消去相同的部分

$$ P(A|B)P(B) = P(B|A)P(A)$$
最终我们就可以得到这个公式，这个公式就是贝叶斯公式。
$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

$ y = f(x)$
- y ： 接收 offer
- x : $x_1$ 表示常出差 $x_2$ 表示常加班 $x_3$ 表示高工资
$$ P(y=1|x_1,x_2,x_3) $$
$$ P(y=0|x_1,x_2,x_3) $$
$$ \begin{cases}
    x_1 = 0 \\
    x_2 = 0 \\
    x_3 = 1 \\
\end{cases} $$
最终我们将这些参数带入到贝叶斯公式
$$ P(y|x_1,x_2,x_3) = \frac{P(x_1x_2x_3|y)P(y)}{P(x_1x_2x_3)} $$
我们无需看分母，因为无论 y = 1 或者 y = 0 他们分母都是一样的。
$P(x_1x_2x_3|y)$ 这部分内容看起来比较难算，可以马可夫进行

$$ P(y|x_1,x_2,x_3) \approx P(x_1|y)P(x_2|y)P(x_3|y)P(y) $$

$$ P(y=1|x_1,x_2,x_3) \approx P(x_1|y)P(x_2|y)P(x_3|y)P(y) $$

$$ P(y=1|x_1,x_2,x_3) \approx P(0|1)P(0|1)P(1|1)P(y) $$

- 接收offer没有出差
$P(0|1)$ 表示已知接收offer了有多少没有出差情况，可以查表得到接收offer没有出差是 $\frac{2}{3}$

- 接收offer没有加班
$P(0|1)$ 表示已知接收offer了有多少没有加班情况，可以查表得到接收offer没有加班是 $\frac{2}{3}$

- 接收offer高薪
$P(0|1)$ 表示已知接收offer了有多少高薪情况，可以查表得到接收offer高薪是 1

- 接收offer概率
  概率是 $\frac{1}{2}$

那么我么就很轻松求取概率
不加班不出差高薪招聘到人的概率
$$ P(y=1|x_1,x_2,x_3) \approx \frac{2}{3}\frac{2}{3} \frac{1}{2} \approx 22%$$

不加班不出差高薪招聘不到人的概率
$$ P(y=0|x_1,x_2,x_3) \approx \frac{2}{3}\frac{2}{3} \frac{1}{3} \frac{1}{2} \approx 7.3%$$

从结果来看，不加班，不出差高薪还是很容易招聘到人


$$ P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)} $$
### 什么是贝叶斯统计

### 拉普拉斯
### 傅里叶
### 拉格朗日
## 假设验证
我们无法对全体样本进行调查，所有需要抽样来推测总体样本的情况
### 概率基础知识
假设标准正态分布 $u=0, \sigma=1$ 
<!-- x 小于等于 1 大于等于 -1 $P$ -->
落在 1 倍标准差内的概率
$$ P(-1 \le x \le 1 ) = 68 \% $$ 
$$ P(-2 \le x \le 2 ) = 95 \% $$
$$ P(-3 \le x \le 3 ) = 99.73 \% $$

### 模特卡洛
模特卡洛是通过比较随机的方法来得到比较确定的方法。
模特卡洛是一个赌场，我们通过经典的实验（计算pi）来帮助我们了解什么是模特卡洛，已经如何应用模特卡洛。
想象平面上一个边长为 2 的正方形，里面内切一个半径为 1 的圆，我们均匀随机地在正方形里撒米粒，那么每一个数一下落子园内和概率就是圆和正方形面积之比
$$ p \frac{圆面积}{正方形面积} = \frac{\pi}{4}$$
$$ \pi \approx 4 * \frac{落在圆内点数}{总点数} $$

![图](https://upload-images.jianshu.io/upload_images/8207483-f3dd7842779399d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
import numpy as np

num_iterations = 10**4

pts = np.random.uniform(low=-1,high=1,size=(num_iterations,2))

sq_radiuses = np.sum(pts**2,axis=1)
in_circle = (sq_radiuses < 1)
approx_pi = 4 * np.sum(in_circle) /(num_iterations) * 1.00000

print(approx_pi)
```

```
3.1332
```

### 伯努利实验(bernouli)
N 次独立抽样，结果是抽象的得到分布，$$ 
随机试验的独立性，简单介绍一下就是两个不相干的事件他们分别都是随机独立试验。例如一个同学投硬币（试验）另一个同学掷骰子（试验)
分别属于这些试验的随机事件都是相互独立的，常常把只有两个可能结果的随机试验称为伯努利(bernoulli)实验。一个结果P(A发生) = p(A不发生) = q = 1 -p。我们可以这个伯努利实验重复多次。
独立重复N次的伯努利试验，通常叫做多重伯努利试验。
- n 次试验中 A事件恰好发生 k 次的概率是多少
- 到第 k 次试验，A 才首次发生的概率是多少

### Beta 分布
就是代表伯努利试验，$ P \approx X^m(1-x)^n$
概率的概率

最大似然估计(MLE)maximum likelihood

让分类准确率最高，分类标签预测概率最大这是我们目标函数，可以推演出平方误差最小。
似然性就是概率就是相似所以，我的假设使我数据出现最大。
$$ P(D|\theta) $$ 
- 数据就是 $$ P((x_1,y_1)(x_2,y_2) \dots (x_n,y_n)| \theta) $$

要理解**极大似然估计**是什么我们需要明白**概率密度(质量)函数**是什么，概率密度函数用来描述某个随机变量取某个值的时候，取值点所对应的概率的函数，概率分布

$$ f(x;\mu \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} \exp (=\frac{(x - \um)^2}{ 2 \sigma^2})  $$
其中有两个参数随机变量 x 对应两个参数分别是 $\mu$ 均值 和 $\sigma$ 标准差。

- 概率: 是在已知一些概率分布参数的情况下，预测观测结果
- 似然: 这是已知某些观测所得到的结果时，对观测结果所属的概率分布的参数进行估值。

估计过程就是极大似然估计，估计就是找到一个最符合当前观测数据的概率分布。
假设观测数据一共有 N 个 ${x_1,x_2, \dots x_N}$ 我们推断这一组随机变量是属于一个概率分布，是一个正态分布，他概率密度函数为$f(x;\mu \sigma)$, 我们把这些样本点带入$f(x;\mu \sigma)$里，得到每一个数据点在我们假设的概率分布中出现的可能性，注意
这是这组数据的条件概率
$$ P(x_1|\mu,\sigma) P(x_2|\mu,\sigma), \dots P(x_N|\mu,\sigma) $$

那么这组数据同时出现概率就是这些点的概率的乘积
$$ L(\mu,\sigma | X) = P(X|\mu, \sigma) = \prod_{i=1}^N P(x_i|\mu, \sigma)$$
其实似然函数也是条件概率，只是我们观察关注的变量值 x 改变为参数值 $\mu$ 和 $\simga$

我们需要
<!-- mean 5 ,sigma -->

对数似然值(log likelihood) 我们对似然函数取 log

$$ L(\um,\sigma|X) = \sum_{i=1}^N \log P(x_i|\mu,\sigma)$$