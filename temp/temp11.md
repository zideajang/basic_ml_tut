## PCA 主成分分析
#### 知识背景
- 协方差
- SVD 分解
- 特征值与特征向量


#### PCA算法两种实现方法
- 基于特征值分解协方差矩阵实现PCA算法
- 基于SVD分解协方差矩阵实现PCA算法

其中，Q是矩阵A的特征向量组成的矩阵，则是一个对角阵，对角线上的元素就是特征值。

我们来分析一下特征值分解的式子，分解得到的Σ矩阵是一个对角矩阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变换方向（从主要的变化到次要的变化排列）。

当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变换可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变化方向，我们通过特征值分解得到的前N个特征向量，就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵变换。也就是之前说的：提取这个矩阵最重要的特征。

总结：特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多么重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

### SVD 计算
有关 SVD 应用很广泛
奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：

$$A = U \sum V^T$$

-  A 是一个 mn 的矩阵
-  U 是一个 mm 的方阵，U里面的正交向量被称为左奇异向量。
-  Σ 是一个m n 的矩阵，Σ 除了对角线其它元素都为0，对角线上的元素称为奇异值。
-  $V^T$ 是v的转置矩阵，是一个n n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列。上面矩阵的维度变化可以参照图4所示。
计算过程就是先求
$$(A^TA)v_i = \lambda v_i $$
$$(AA^T)\mu_i = \lambda \mu_i$$


$$ \left( \begin{matrix}
    0 & 1 \\
    1 & 1 \\
    1 & 0 
\end{matrix} \right)$$

$$A^TA = \left( \begin{matrix}
    0 & 1 & 1 \\
    1 & 1 & 0 \\
\end{matrix} \right) \left(\begin{matrix}
    0 & 1 \\
    1 & 1 \\
    1 & 0 
\end{matrix} \right) = \left(\begin{matrix}
    2 & 1 \\
    1 & 2 
\end{matrix} \right)$$

$$AA^T =  \left(\begin{matrix}
    0 & 1 \\
    1 & 1 \\
    1 & 0 
\end{matrix} \right) \left( \begin{matrix}
    0 & 1 & 1 \\
    1 & 1 & 0 \\
\end{matrix} \right) = \left(\begin{matrix}
    1 & 1 & 0 \\
    1 & 2 & 1 \\
    0 & 1 & 1 
\end{matrix} \right)$$